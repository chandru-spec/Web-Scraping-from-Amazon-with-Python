{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Web Scraping from Amazon with Python**"
      ],
      "metadata": {
        "id": "MmWFwAdN9P8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping is a technique used to extract data from websites by sending requests to the server, retrieving the web pages, and parsing the HTML content to extract the necessary information. If you are learning Data Science, you should know how to collect data from APIs or Web Scraping. So, in this article, I’ll take you through a step-by-step tutorial on data collection from Amazon using web scraping with Python."
      ],
      "metadata": {
        "id": "o_z3MUah-d2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I will walk through the process of web scraping from Amazon’s Best Sellers page in the Teaching & Education category to collect data about the top 50 authors and their ratings. Before we start, ensure you have the following Python libraries installed:\n",
        "\n",
        "requests: to send HTTP requests and retrieve the web pages.\n",
        "BeautifulSoup: to parse and extract information from the HTML content.\n",
        "pandas: to organize and save the extracted data in a tabular format.\n",
        "Pandas and requests are already available in your Python environment if you are using colab or jupyter notebook. Use the command below in your colab or jupyter notebook to install BeautifulSoup:"
      ],
      "metadata": {
        "id": "CpAqIdRq-1jN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDvKf1aR6hzM",
        "outputId": "f70e856c-238b-40d6-a3dc-71729cc11cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Understanding the Target URL and Pagination\n",
        "We are targeting the Amazon Best Sellers page in the Teaching & Education category. Amazon’s pagination allows us to navigate through multiple pages of results. The base URL for the first page looks\n",
        "\n",
        "LINK: https://www.amazon.in/gp/bestsellers/books/4149461031/ref=zg_bs_pg_1?ie=UTF8&pg=1"
      ],
      "metadata": {
        "id": "R3909g75_D4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2:** Set Up the HTTP Request\n",
        "\n",
        "To scrape the content from Amazon, we first need to send a request to the server and retrieve the HTML content of the page. We also need to mimic a real browser to avoid being blocked by Amazon, which is why we always need to include a User-Agent header in the request. Here’s how to set up the HTTP request:"
      ],
      "metadata": {
        "id": "eUcyjHsM_SvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# base url of the best sellers page for teaching & education books\n",
        "base_url = \"https://www.amazon.in/gp/bestsellers/books/4149461031/ref=zg_bs_pg_{}?ie=UTF8&pg={}\"\n",
        "\n",
        "# http headers to mimic a browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "}"
      ],
      "metadata": {
        "id": "2_qI3N__62nN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3**: Iterate Over Pages to Collect Data\n",
        "Now, we will loop through the first three pages to collect data for the top 50 books (assuming each page displays around 20 items). On each page, we will extract the author’s name and rating."
      ],
      "metadata": {
        "id": "RIr8U6-t_cOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a list to store book data\n",
        "book_list = []\n",
        "\n",
        "# iterate over the first 3 pages to get top 50 books (assuming each page has about 20 items)\n",
        "for page in range(1, 4):\n",
        "    # construct the URL for the current page\n",
        "    url = base_url.format(page, page)\n",
        "\n",
        "    # send a GET request to the url\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"lxml\")\n",
        "\n",
        "    # find all the book elements\n",
        "    books = soup.find_all(\"div\", {\"class\": \"zg-grid-general-faceout\"})\n",
        "\n",
        "    # iterate over each book element to extract data\n",
        "    for book in books:\n",
        "        if len(book_list) < 50:  # stop once we've collected 50 books\n",
        "            author = book.find(\"a\", class_=\"a-size-small a-link-child\").get_text(strip=True) if book.find(\"a\", class_=\"a-size-small a-link-child\") else \"N/A\"\n",
        "            rating = book.find(\"span\", class_=\"a-icon-alt\").get_text(strip=True) if book.find(\"span\", class_=\"a-icon-alt\") else \"N/A\"\n",
        "\n",
        "            # append the extracted data to the book_list\n",
        "            book_list.append({\n",
        "                \"Author\": author,\n",
        "                \"Rating\": rating\n",
        "            })\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "Za-zhE1V7JDQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we looped through the first three pages using a for loop. The condition if len(book_list) < 50: ensures that we stop once we’ve collected data for 50 books. The code works by iterating through the first three pages of Amazon’s Best Sellers list in the Teaching & Education category. For each page, it sends a GET request to retrieve the HTML content, then parses this content using BeautifulSoup to find the relevant book elements. It extracts the author and rating for each book, appending the data to a list until it has collected information for 50 books."
      ],
      "metadata": {
        "id": "9icO_hf-_odA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loop breaks once 50 books have been processed, which ensures that only the top 50 authors and their ratings are captured."
      ],
      "metadata": {
        "id": "mBKZDkKf_wkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4:** Store and Save the Data\n",
        "After collecting the data, we will store it in a Pandas DataFrame and save it to a CSV file:"
      ],
      "metadata": {
        "id": "b9bcSiYg_zwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the list of dictionaries into a DataFrame\n",
        "df = pd.DataFrame(book_list)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "df.to_csv(\"amazon_top_50_books_authors_ratings.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34iSxqix71qb",
        "outputId": "2d7d8187-ddec-45f6-93c8-2fc45aafcc40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Author              Rating\n",
            "0  Samapti Sinha Mahapatra  4.6 out of 5 stars\n",
            "1             Kriti Sharma  4.7 out of 5 stars\n",
            "2                 Kautilya  4.5 out of 5 stars\n",
            "3        Ishinna B. Sadana  4.8 out of 5 stars\n",
            "4                 PR Yadav  4.4 out of 5 stars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.sample(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ULCb9A8CbT",
        "outputId": "7bc6d7a4-8ea8-4b42-8074-d9d9dd44a763"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Author              Rating\n",
            "3    Ishinna B. Sadana  4.8 out of 5 stars\n",
            "29                 N/A  4.2 out of 5 stars\n",
            "9      Esther Wojcicki  4.5 out of 5 stars\n",
            "8           R.K. Gupta  4.5 out of 5 stars\n",
            "41                 N/A  5.0 out of 5 stars\n",
            "28     Nikhil Kr Gupta  4.5 out of 5 stars\n",
            "25       MANJUNATH M S  4.6 out of 5 stars\n",
            "13  Wonder House Books  4.7 out of 5 stars\n",
            "15                 N/A                 N/A\n",
            "1         Kriti Sharma  4.7 out of 5 stars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method can be adapted for different categories or more extensive data collection by adjusting the page range or the conditions within the loop."
      ],
      "metadata": {
        "id": "QnL6KBFMAMaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CONCLUSION:**\n",
        "So, web scraping is a technique used to extract data from websites by sending requests to the server, retrieving the web pages, and parsing the HTML content to extract the necessary information. I hope you liked this article on data collection using Web Scraping from Amazon with Python. Feel free to ask valuable questions in the comments section below. You can follow me on Instagram for many more resources."
      ],
      "metadata": {
        "id": "2UY1e6lYAaVy"
      }
    }
  ]
}